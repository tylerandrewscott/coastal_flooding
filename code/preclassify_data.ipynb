{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import scipy.io\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.optimizers import SGD, Adadelta, Adagrad\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from six.moves import range\n",
    "import keras.utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import tokenizer_from_json\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import io\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = '../input/floodingtweetsforvalidation.csv'\n",
    "df = pd.read_csv(fname, engine='python')\n",
    "full_texts = [t for t in df['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5715 unique tokens.\n",
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "with open('../model_objects/tokenizer_V1.json') as f: \n",
    "        data = json.load(f) \n",
    "        tokenizer = tokenizer_from_json(data)\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "sequences = tokenizer.texts_to_sequences(full_texts)\n",
    "new_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(new_word_index))\n",
    "\n",
    "new_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('../model_objects/flood_classifier_model_V1.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"../model_objects/flood_classifer_model_V1.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "preds = loaded_model.predict(new_data)\n",
    "df['Probability_Flooding_V1'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26251 unique tokens.\n",
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "with open('../model_objects/tokenizer_V2.json') as f: \n",
    "        data = json.load(f) \n",
    "        tokenizer = tokenizer_from_json(data)\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "sequences = tokenizer.texts_to_sequences(full_texts)\n",
    "new_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(new_word_index))\n",
    "\n",
    "new_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('../model_objects/flood_classifier_model_V2.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"../model_objects/flood_classifer_model_V2.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "preds = loaded_model.predict(new_data)\n",
    "df['Probability_Flooding_V2'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../input/floodingtweets_predictedprobability_comparison.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
