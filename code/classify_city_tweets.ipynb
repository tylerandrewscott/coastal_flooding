{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import scipy.io\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.optimizers import SGD, Adadelta, Adagrad\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from six.moves import range\n",
    "import keras.utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import tokenizer_from_json\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import io\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "import os.path\n",
    "from os import path\n",
    "sloc = '../input/coastal_city_by_day/'\n",
    "ddir = '../../Second Twitter Flooding Paper/Data/Daily Tweets Coastal Cities/'\n",
    "flist = os.listdir(ddir)\n",
    "from itertools import compress\n",
    "exists = [path.exists(sloc + f)==False for f in flist]\n",
    "flist = list(compress(flist, exists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26251 unique tokens.\n",
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "with open('../model_objects/tokenizer_V2.json') as f: \n",
    "        data = json.load(f) \n",
    "        tokenizer = tokenizer_from_json(data)\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "new_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(new_word_index))\n",
    "        \n",
    "# load json and create model\n",
    "json_file = open('../model_objects/flood_classifier_model_V2.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"../model_objects/flood_classifer_model_V2.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_predict_values(file):\n",
    "    ddir = '../../Second Twitter Flooding Paper/Data/Daily Tweets Coastal Cities/'\n",
    "    new_name = file\n",
    "    fname =  ddir + file\n",
    "    #print(fname)\n",
    "    df = pd.read_csv(fname, engine='python')\n",
    "    df = df[df['text'].notna()]\n",
    "    full_texts = [t for t in df['text']]\n",
    "    sequences = tokenizer.texts_to_sequences(full_texts)\n",
    "    new_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    preds = loaded_model.predict(new_data)\n",
    "    df['Probability_Flooding_V2'] = preds\n",
    "    df.to_csv('../input/coastal_city_by_day/'+os.path.basename(fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2019_12_11.csv', '2019_12_12.csv', '2019_12_13.csv', '2019_12_14.csv']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in flist:\n",
    "    f_predict_values(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (workerpool.py, line 173)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3331\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-16-569a8eb083b9>\"\u001b[0m, line \u001b[1;32m3\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    import workers\n",
      "\u001b[0;36m  File \u001b[0;32m\"/opt/anaconda3/lib/python3.7/site-packages/workers/__init__.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from . workerpool import *\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/opt/anaconda3/lib/python3.7/site-packages/workers/workerpool.py\"\u001b[0;36m, line \u001b[0;32m173\u001b[0m\n\u001b[0;31m    except Exception, e:\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from multiprocessing import Pool\n",
    "import workers\n",
    "if __name__ ==  '__main__': \n",
    "    num_processors = 4\n",
    "    p=Pool(processes = num_processors)\n",
    "    output = p.map(workers.worker,flist[:20])\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
