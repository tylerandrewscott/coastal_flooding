{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "Found 2000 texts.\n",
      "Found 5715 unique tokens.\n",
      "Shape of data tensor: (2000, 100)\n",
      "Shape of label tensor: (2000,)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import scipy.io\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.optimizers import SGD, Adadelta, Adagrad\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from six.moves import range\n",
    "import keras.utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "\n",
    "BASE_DIR = '../'\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, '../glove.6B')\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, 'input')\n",
    "MAX_SEQUENCE_LENGTH =100\n",
    "MAX_NUM_WORDS = 100000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "coded_df = pd.read_csv(os.path.join(TEXT_DATA_DIR,'balanced_trainers_V1.csv'),engine = 'python')\n",
    "\n",
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "\n",
    "label_id = range(coded_df.shape[0])\n",
    "labels_index = 1\n",
    "texts = [t for t in coded_df['Tweet']]\n",
    "labels = [y for y in coded_df['Y']]\n",
    "\n",
    "print('Found %s texts.' % len(texts))\n",
    "\n",
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.asarray(labels)\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_json = tokenizer.to_json() \n",
    "with io.open('../model_objects/tokenizer_V1.json', 'w', encoding='utf-8') as f:  \n",
    "      f.write(json.dumps(tokenizer_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Preparing embedding matrix.\n",
      "Training model.\n"
     ]
    }
   ],
   "source": [
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "print('Preparing embedding matrix.')\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "\n",
    "\n",
    "print('Training model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(GRU(units = 32,dropout = 0.2,recurrent_dropout = 0.2))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy',optimizer = 'adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/20\n",
      " - 2s - loss: 0.7077 - accuracy: 0.5256 - val_loss: 0.6813 - val_accuracy: 0.5775\n",
      "Epoch 2/20\n",
      " - 1s - loss: 0.6582 - accuracy: 0.5950 - val_loss: 0.6578 - val_accuracy: 0.6025\n",
      "Epoch 3/20\n",
      " - 1s - loss: 0.6023 - accuracy: 0.6981 - val_loss: 0.6298 - val_accuracy: 0.6450\n",
      "Epoch 4/20\n",
      " - 1s - loss: 0.5540 - accuracy: 0.7550 - val_loss: 0.6053 - val_accuracy: 0.6950\n",
      "Epoch 5/20\n",
      " - 1s - loss: 0.5019 - accuracy: 0.7862 - val_loss: 0.5913 - val_accuracy: 0.6850\n",
      "Epoch 6/20\n",
      " - 1s - loss: 0.4574 - accuracy: 0.8156 - val_loss: 0.5699 - val_accuracy: 0.7025\n",
      "Epoch 7/20\n",
      " - 1s - loss: 0.4393 - accuracy: 0.8194 - val_loss: 0.5739 - val_accuracy: 0.6725\n",
      "Epoch 8/20\n",
      " - 1s - loss: 0.4162 - accuracy: 0.8313 - val_loss: 0.5603 - val_accuracy: 0.6925\n",
      "Epoch 9/20\n",
      " - 1s - loss: 0.3987 - accuracy: 0.8250 - val_loss: 0.5395 - val_accuracy: 0.7175\n",
      "Epoch 10/20\n",
      " - 1s - loss: 0.3975 - accuracy: 0.8350 - val_loss: 0.5422 - val_accuracy: 0.7025\n",
      "Epoch 11/20\n",
      " - 1s - loss: 0.3795 - accuracy: 0.8475 - val_loss: 0.5256 - val_accuracy: 0.7300\n",
      "Epoch 12/20\n",
      " - 1s - loss: 0.3668 - accuracy: 0.8350 - val_loss: 0.5070 - val_accuracy: 0.7650\n",
      "Epoch 13/20\n",
      " - 1s - loss: 0.3672 - accuracy: 0.8550 - val_loss: 0.5066 - val_accuracy: 0.7625\n",
      "Epoch 14/20\n",
      " - 1s - loss: 0.3551 - accuracy: 0.8525 - val_loss: 0.5059 - val_accuracy: 0.7475\n",
      "Epoch 15/20\n",
      " - 1s - loss: 0.3412 - accuracy: 0.8562 - val_loss: 0.5004 - val_accuracy: 0.7525\n",
      "Epoch 16/20\n",
      " - 1s - loss: 0.3468 - accuracy: 0.8581 - val_loss: 0.5033 - val_accuracy: 0.7500\n",
      "Epoch 17/20\n",
      " - 1s - loss: 0.3418 - accuracy: 0.8494 - val_loss: 0.4867 - val_accuracy: 0.7850\n",
      "Epoch 18/20\n",
      " - 1s - loss: 0.3350 - accuracy: 0.8687 - val_loss: 0.4900 - val_accuracy: 0.7575\n",
      "Epoch 19/20\n",
      " - 1s - loss: 0.3261 - accuracy: 0.8656 - val_loss: 0.4961 - val_accuracy: 0.7575\n",
      "Epoch 20/20\n",
      " - 1s - loss: 0.3189 - accuracy: 0.8712 - val_loss: 0.4892 - val_accuracy: 0.7575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1411cee50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=20,\n",
    "          validation_data=(x_val, y_val),verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "#x = MaxPooling1D(5)(x)\n",
    "#x = Conv1D(128, 5, activation='relu')(x)\n",
    "#x = MaxPooling1D(5)(x)\n",
    "#x = Conv1D(128, 5, activation='relu')(x)\n",
    "#x = GlobalMaxPooling1D()(x)\n",
    "#x = Dense(128,activation='relu')(x)\n",
    "#preds = Dense(labels_index, activation='softmax')(x)\n",
    "\n",
    "#model = Model(sequence_input, preds)\n",
    "#model.compile(loss='binary_crossentropy',\n",
    "              #optimizer='rmsprop',\n",
    "#              optimizer = Adam(0.01),\n",
    "#             metrics=['acc'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#%mkdir ../model_objects\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"../model_objects/flood_classifier_model_V1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"../model_objects/flood_classifer_model_V1.h5\")\n",
    "print(\"Saved model to disk\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5716\n"
     ]
    }
   ],
   "source": [
    "print(num_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
