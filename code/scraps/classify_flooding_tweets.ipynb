{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5715 unique tokens.\n",
      "Shape of data tensor: (2000, 125)\n",
      "Shape of label tensor: (2000,)\n",
      "Found 400000 word vectors.\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/30\n",
      "1600/1600 [==============================] - 0s 194us/step - loss: 0.5565 - accuracy: 0.7469 - val_loss: 0.4534 - val_accuracy: 0.8525\n",
      "Epoch 2/30\n",
      "1600/1600 [==============================] - 0s 98us/step - loss: 0.3996 - accuracy: 0.8531 - val_loss: 0.3572 - val_accuracy: 0.8700\n",
      "Epoch 3/30\n",
      "1600/1600 [==============================] - 0s 102us/step - loss: 0.3428 - accuracy: 0.8606 - val_loss: 0.3350 - val_accuracy: 0.8850\n",
      "Epoch 4/30\n",
      "1600/1600 [==============================] - 0s 99us/step - loss: 0.3124 - accuracy: 0.8769 - val_loss: 0.3186 - val_accuracy: 0.8850\n",
      "Epoch 5/30\n",
      "1600/1600 [==============================] - 0s 132us/step - loss: 0.2859 - accuracy: 0.8875 - val_loss: 0.3100 - val_accuracy: 0.8900\n",
      "Epoch 6/30\n",
      "1600/1600 [==============================] - 0s 100us/step - loss: 0.2664 - accuracy: 0.8975 - val_loss: 0.3018 - val_accuracy: 0.8975\n",
      "Epoch 7/30\n",
      "1600/1600 [==============================] - 0s 85us/step - loss: 0.2459 - accuracy: 0.9038 - val_loss: 0.2909 - val_accuracy: 0.8925\n",
      "Epoch 8/30\n",
      "1600/1600 [==============================] - 0s 92us/step - loss: 0.2291 - accuracy: 0.9219 - val_loss: 0.2868 - val_accuracy: 0.9000\n",
      "Epoch 9/30\n",
      "1600/1600 [==============================] - 0s 104us/step - loss: 0.2172 - accuracy: 0.9194 - val_loss: 0.2723 - val_accuracy: 0.9100\n",
      "Epoch 10/30\n",
      "1600/1600 [==============================] - 0s 99us/step - loss: 0.2012 - accuracy: 0.9287 - val_loss: 0.2778 - val_accuracy: 0.9125\n",
      "Epoch 11/30\n",
      "1600/1600 [==============================] - 0s 131us/step - loss: 0.1794 - accuracy: 0.9425 - val_loss: 0.2700 - val_accuracy: 0.9150\n",
      "Epoch 12/30\n",
      "1600/1600 [==============================] - 0s 111us/step - loss: 0.1753 - accuracy: 0.9394 - val_loss: 0.2727 - val_accuracy: 0.9100\n",
      "Epoch 13/30\n",
      "1600/1600 [==============================] - 0s 102us/step - loss: 0.1572 - accuracy: 0.9550 - val_loss: 0.2589 - val_accuracy: 0.9250\n",
      "Epoch 14/30\n",
      "1600/1600 [==============================] - 0s 113us/step - loss: 0.1415 - accuracy: 0.9606 - val_loss: 0.2540 - val_accuracy: 0.9300\n",
      "Epoch 15/30\n",
      "1600/1600 [==============================] - 0s 82us/step - loss: 0.1325 - accuracy: 0.9606 - val_loss: 0.2498 - val_accuracy: 0.9275\n",
      "Epoch 16/30\n",
      "1600/1600 [==============================] - 0s 98us/step - loss: 0.1179 - accuracy: 0.9669 - val_loss: 0.2488 - val_accuracy: 0.9350\n",
      "Epoch 17/30\n",
      "1600/1600 [==============================] - 0s 116us/step - loss: 0.1096 - accuracy: 0.9737 - val_loss: 0.2563 - val_accuracy: 0.9325\n",
      "Epoch 18/30\n",
      "1600/1600 [==============================] - 0s 115us/step - loss: 0.0997 - accuracy: 0.9744 - val_loss: 0.2475 - val_accuracy: 0.9425\n",
      "Epoch 19/30\n",
      "1600/1600 [==============================] - 0s 97us/step - loss: 0.0913 - accuracy: 0.9781 - val_loss: 0.2463 - val_accuracy: 0.9475\n",
      "Epoch 20/30\n",
      "1600/1600 [==============================] - 0s 105us/step - loss: 0.0864 - accuracy: 0.9775 - val_loss: 0.2545 - val_accuracy: 0.9450\n",
      "Epoch 21/30\n",
      "1600/1600 [==============================] - 0s 108us/step - loss: 0.0798 - accuracy: 0.9800 - val_loss: 0.2587 - val_accuracy: 0.9400\n",
      "Epoch 22/30\n",
      "1600/1600 [==============================] - 0s 101us/step - loss: 0.0728 - accuracy: 0.9844 - val_loss: 0.2599 - val_accuracy: 0.9475\n",
      "Epoch 23/30\n",
      "1600/1600 [==============================] - 0s 90us/step - loss: 0.0666 - accuracy: 0.9844 - val_loss: 0.2568 - val_accuracy: 0.9425\n",
      "Epoch 24/30\n",
      "1600/1600 [==============================] - 0s 109us/step - loss: 0.0644 - accuracy: 0.9850 - val_loss: 0.2520 - val_accuracy: 0.9475\n",
      "Epoch 25/30\n",
      "1600/1600 [==============================] - 0s 118us/step - loss: 0.0664 - accuracy: 0.9806 - val_loss: 0.2536 - val_accuracy: 0.9425\n",
      "Epoch 26/30\n",
      "1600/1600 [==============================] - 0s 108us/step - loss: 0.0584 - accuracy: 0.9887 - val_loss: 0.2694 - val_accuracy: 0.9425\n",
      "Epoch 27/30\n",
      "1600/1600 [==============================] - 0s 119us/step - loss: 0.0516 - accuracy: 0.9881 - val_loss: 0.2704 - val_accuracy: 0.9450\n",
      "Epoch 28/30\n",
      "1600/1600 [==============================] - 0s 122us/step - loss: 0.0490 - accuracy: 0.9894 - val_loss: 0.2762 - val_accuracy: 0.9450\n",
      "Epoch 29/30\n",
      "1600/1600 [==============================] - 0s 90us/step - loss: 0.0468 - accuracy: 0.9919 - val_loss: 0.2686 - val_accuracy: 0.9475\n",
      "Epoch 30/30\n",
      "1600/1600 [==============================] - 0s 115us/step - loss: 0.0445 - accuracy: 0.9912 - val_loss: 0.2664 - val_accuracy: 0.9475\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import scipy.io\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adadelta, Adagrad\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from six.moves import range\n",
    "import keras.utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "auto_coded_df = pd.read_csv('../input/balanced_trainers.csv')\n",
    "y = np.array([y for y in auto_coded_df['Y']])\n",
    "auto_texts = list(auto_coded_df['Tweet'])\n",
    "\n",
    "texts = auto_texts\n",
    "\n",
    "MAX_NUM_WORDS = 100\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "GLOVE_DIR = '../../wqvectors/glove.6B/'\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "MAX_SEQUENCE_LENGTH = len(max(sequences, key=len)) + 100\n",
    "#MAX_SEQUENCE_LENGTH = 1000\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = y\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "shuffle_split = False\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "import os\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import Dense, Embedding, GlobalMaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "h = GlobalMaxPooling1D()(embedded_sequences)\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(h)\n",
    "\n",
    "model = Model(inputs=sequence_input, outputs=output)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(0.01),\n",
    "              metrics=['accuracy'])\n",
    "hist = model.fit(x_train, y_train, epochs=30, batch_size=100, validation_data=(x_val, y_val))\n",
    "#model.save('../model_objects/topic_classifier.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
